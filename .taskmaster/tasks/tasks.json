{
  "master": {
    "tasks": [
      {
        "id": 1,
        "title": "Set up Vercel AI SDK Provider Configuration",
        "description": "Create unified provider configuration for Anthropic (Sonnet 4.5), Custom Perplexity (Sonar Deep Research), and Ollama wrapper to establish the foundation for AI SDK integration",
        "details": "Create three provider files: 1) `frontend/lib/ai/providers/anthropicProvider.ts` - Configure Anthropic provider with Sonnet 4.5 model, environment validation, and type-safe model selection. 2) `frontend/lib/ai/providers/perplexityProvider.ts` - Create custom Perplexity provider using OpenAI-compatible adapter pointing to api.perplexity.ai with Sonar Deep Research model. 3) `frontend/lib/ai/providers/ollamaProvider.ts` - Wrap existing OllamaClient to maintain compatibility as emergency fallback. 4) `frontend/lib/ai/config.ts` - Central configuration with provider selection, model configs, retry/timeout settings, and feature flags. Use latest @ai-sdk/anthropic ^1.0.0 and ensure proper error handling for missing API keys.",
        "testStrategy": "Unit tests for provider initialization, environment variable validation, model selection type safety, and error handling for missing credentials. Integration tests for basic connectivity to each provider.",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Create Anthropic Provider Configuration",
            "description": "Set up the Anthropic provider with Sonnet 4.5 model configuration, environment validation, and type-safe model selection",
            "dependencies": [],
            "details": "Create `frontend/lib/ai/providers/anthropicProvider.ts` file implementing: 1) Import @ai-sdk/anthropic provider with proper TypeScript types. 2) Configure Sonnet 4.5 model with appropriate settings (temperature, max tokens, etc.). 3) Add environment variable validation for ANTHROPIC_API_KEY with clear error messages. 4) Implement type-safe model selection interface. 5) Add proper error handling for initialization failures and API key issues. 6) Export configured provider instance with documentation.",
            "status": "pending",
            "testStrategy": "Unit tests for provider initialization, environment variable validation, model configuration correctness, and error handling for missing/invalid API keys"
          },
          {
            "id": 2,
            "title": "Create Custom Perplexity Provider",
            "description": "Build custom Perplexity provider using OpenAI-compatible adapter for Sonar Deep Research model integration",
            "dependencies": [],
            "details": "Create `frontend/lib/ai/providers/perplexityProvider.ts` implementing: 1) Use OpenAI-compatible adapter pattern pointing to api.perplexity.ai endpoint. 2) Configure Sonar Deep Research model with proper parameters. 3) Add PERPLEXITY_API_KEY environment validation with descriptive error messages. 4) Implement request/response transformation to match AI SDK patterns. 5) Add proper error handling for API connectivity and authentication issues. 6) Export provider with clear TypeScript interface definitions.",
            "status": "pending",
            "testStrategy": "Unit tests for adapter configuration, API endpoint setup, environment validation, request/response transformation accuracy, and connectivity error handling"
          },
          {
            "id": 3,
            "title": "Create Ollama Provider Wrapper",
            "description": "Wrap existing OllamaClient to maintain compatibility with AI SDK patterns as emergency fallback provider",
            "dependencies": [],
            "details": "Create `frontend/lib/ai/providers/ollamaProvider.ts` implementing: 1) Import and wrap existing OllamaClient from current implementation. 2) Create adapter layer to match AI SDK provider interface patterns. 3) Maintain backward compatibility with existing Ollama configuration. 4) Add proper error handling and fallback behavior. 5) Implement health check functionality for local Ollama availability. 6) Export wrapped provider with consistent interface matching other providers.",
            "status": "pending",
            "testStrategy": "Unit tests for wrapper functionality, interface compatibility, backward compatibility validation, health check accuracy, and fallback behavior testing"
          },
          {
            "id": 4,
            "title": "Create Central AI Configuration System",
            "description": "Build unified configuration system for provider selection, model settings, retry logic, and feature flags",
            "dependencies": [
              1,
              2,
              3
            ],
            "details": "Create `frontend/lib/ai/config.ts` implementing: 1) Central configuration object with provider selection logic (primary/fallback hierarchy). 2) Model configuration settings for each provider (temperature, max tokens, timeout values). 3) Retry and timeout configuration with exponential backoff settings. 4) Feature flags for gradual rollout and A/B testing capabilities. 5) Environment-based configuration switching (dev/staging/prod). 6) Type-safe configuration validation and error handling. 7) Export unified config interface for use across the application.",
            "status": "pending",
            "testStrategy": "Unit tests for configuration validation, provider selection logic, feature flag functionality, environment switching, and configuration merging behavior"
          }
        ]
      },
      {
        "id": 2,
        "title": "Migrate Zod Schemas for AI SDK Compatibility",
        "description": "Convert existing validation schemas to work with AI SDK's experimental_output feature for structured generation",
        "details": "Create two schema files: 1) `frontend/lib/ai/schemas/questionSchema.ts` - Define schemas for all 27+ question input types (radio_pills, checkbox_cards, scales, sliders, etc.), section structure with 5-7 questions, and complete response with exactly 10 sections totaling 50-70 questions. Include validation for IDs (q1_s1 format), required fields, and metadata. 2) `frontend/lib/ai/schemas/blueprintSchema.ts` - Define comprehensive blueprint schema with all 11 sections (metadata, executive_summary, learning_objectives, etc.), displayType validation for each section, and nested object structures matching current output format. Both schemas must be compatible with AI SDK's experimental_output parameter and provide proper TypeScript type inference.",
        "testStrategy": "Unit tests validating schema compliance with existing data structures, edge cases for all question types, blueprint section validation, and AI SDK experimental_output compatibility. Test with real production data samples.",
        "priority": "high",
        "dependencies": [
          1
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Create Base Schema Types and Utilities",
            "description": "Establish foundational types and utility functions for AI SDK schema compatibility",
            "dependencies": [],
            "details": "Create `frontend/lib/ai/schemas/types.ts` with core TypeScript interfaces for question types, blueprint sections, and AI SDK compatibility. Include utility functions for schema transformation, validation helpers, and type guards. Define base interfaces for QuestionType, SectionStructure, BlueprintSection, and AISDKOutput that will be used across both schema files.",
            "status": "pending",
            "testStrategy": "Unit tests for type validation, utility function correctness, and TypeScript compilation checks"
          },
          {
            "id": 2,
            "title": "Implement Question Schema with All 27+ Input Types",
            "description": "Create comprehensive question schema file supporting all existing question input types and section structures",
            "dependencies": [
              1
            ],
            "details": "Build `frontend/lib/ai/schemas/questionSchema.ts` with Zod schemas for all 27+ question types (radio_pills, checkbox_cards, scales, sliders, currency, multi_select, etc.). Define section structure schema with 5-7 questions per section, complete response schema with exactly 10 sections totaling 50-70 questions. Include ID validation (q1_s1 format), required field validation, metadata schemas, and AI SDK experimental_output compatibility.",
            "status": "pending",
            "testStrategy": "Comprehensive unit tests for each question type validation, section structure compliance, ID format validation, and compatibility with existing dynamic question data"
          },
          {
            "id": 3,
            "title": "Implement Blueprint Schema with 11 Sections",
            "description": "Create comprehensive blueprint schema supporting all blueprint sections and nested structures",
            "dependencies": [
              1
            ],
            "details": "Build `frontend/lib/ai/schemas/blueprintSchema.ts` with complete Zod schema for all 11 blueprint sections (metadata, executive_summary, learning_objectives, curriculum_structure, resources, assessments, implementation_strategy, prerequisites, success_metrics, timeline, conclusion). Include displayType validation, nested object structures, and ensure compatibility with AI SDK experimental_output parameter and existing blueprint JSON format.",
            "status": "pending",
            "testStrategy": "Unit tests for each blueprint section validation, nested structure compliance, displayType validation, and compatibility with existing blueprint data formats"
          },
          {
            "id": 4,
            "title": "Add AI SDK Experimental Output Integration",
            "description": "Integrate schemas with AI SDK experimental_output feature and ensure proper TypeScript inference",
            "dependencies": [
              2,
              3
            ],
            "details": "Update both schema files to work seamlessly with AI SDK's experimental_output parameter. Add proper TypeScript type inference helpers, schema composition utilities, and conversion functions between internal formats and AI SDK requirements. Create adapter functions to transform schemas for different AI providers (Perplexity, Ollama) while maintaining compatibility.",
            "status": "pending",
            "testStrategy": "Integration tests with AI SDK experimental_output, TypeScript inference validation, schema transformation testing, and provider compatibility verification"
          },
          {
            "id": 5,
            "title": "Implement Migration Validation and Backward Compatibility",
            "description": "Ensure schemas maintain backward compatibility with existing data and provide migration validation",
            "dependencies": [
              2,
              3,
              4
            ],
            "details": "Create validation utilities to ensure new schemas work with existing production data. Build migration helpers in `frontend/lib/ai/schemas/migration.ts` for any format changes, compatibility checkers for old vs new data formats, and validation functions to ensure no data loss during migration. Add comprehensive error handling and fallback mechanisms for schema validation failures.",
            "status": "pending",
            "testStrategy": "Migration validation tests with real production data samples, backward compatibility verification, error handling testing, and performance impact assessment of new schemas"
          }
        ]
      },
      {
        "id": 3,
        "title": "Build Question Generation Service with Streaming",
        "description": "Rewrite dynamic question generation service using AI SDK streamObject() with Anthropic primary and Ollama fallback",
        "details": "Create `frontend/lib/ai/services/questionGenerationService.ts` implementing: 1) Use AI SDK's streamObject() with Anthropic Sonnet 4.5 as primary provider. 2) Support both streaming and non-streaming modes with proper TypeScript interfaces. 3) Fallback to Ollama wrapper on Anthropic failure with structured logging. 4) Integrate with question schema for validation and structured outputs. 5) Extract and migrate prompt building logic from existing dynamicQuestionGenerationV2.ts and buildUserPromptV2(). 6) Implement telemetry tracking for token usage, duration, and provider selection. 7) Add comprehensive error handling with standardized error types. Replace custom fetch() calls and retry logic with AI SDK built-in capabilities.",
        "testStrategy": "Integration tests for both streaming and non-streaming modes, fallback scenario testing with mocked Anthropic failures, schema validation testing, token tracking accuracy, and performance benchmarking against current implementation.",
        "priority": "high",
        "dependencies": [
          2
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Set up AI SDK streamObject infrastructure and types",
            "description": "Create the foundational infrastructure for streaming question generation with proper TypeScript interfaces and configuration",
            "dependencies": [],
            "details": "Create `frontend/lib/ai/services/questionGenerationService.ts` with TypeScript interfaces for streaming and non-streaming modes. Define service configuration types, streaming response handlers, and base service structure. Set up proper imports for AI SDK streamObject and establish the foundational architecture for the question generation service.",
            "status": "pending",
            "testStrategy": "Unit tests for interface definitions, service initialization, and configuration validation"
          },
          {
            "id": 2,
            "title": "Implement Anthropic Sonnet 4.5 primary provider integration",
            "description": "Integrate AI SDK streamObject with Anthropic Sonnet 4.5 as the primary question generation provider",
            "dependencies": [
              1
            ],
            "details": "Implement the primary provider logic using AI SDK's streamObject with Anthropic Sonnet 4.5. Configure proper model parameters, streaming handlers, and response processing. Ensure compatibility with the question schema validation and implement structured output generation using AI SDK's experimental_output feature.",
            "status": "pending",
            "testStrategy": "Integration tests for Anthropic provider connectivity, streaming response handling, and schema validation"
          },
          {
            "id": 3,
            "title": "Build Ollama fallback wrapper with error handling",
            "description": "Implement fallback logic to Ollama wrapper when Anthropic fails with comprehensive error handling and logging",
            "dependencies": [
              2
            ],
            "details": "Create fallback mechanism that detects Anthropic failures and seamlessly switches to Ollama wrapper. Implement structured logging for provider selection, error tracking, and failure reasons. Ensure the fallback maintains the same interface and output format as the primary provider while handling different error scenarios gracefully.",
            "status": "pending",
            "testStrategy": "Fallback scenario testing with mocked Anthropic failures, error handling validation, and logging accuracy"
          },
          {
            "id": 4,
            "title": "Extract and migrate prompt building logic",
            "description": "Extract prompt building logic from existing dynamicQuestionGenerationV2.ts and buildUserPromptV2() functions",
            "dependencies": [
              3
            ],
            "details": "Migrate the prompt construction logic from the existing system including user context extraction, section formatting, and question type specifications. Ensure the migrated prompts work with AI SDK's structured generation while maintaining compatibility with the established question formats and user context patterns.",
            "status": "pending",
            "testStrategy": "Comparison testing between old and new prompt outputs, validation of question quality and format consistency"
          },
          {
            "id": 5,
            "title": "Implement telemetry tracking and monitoring",
            "description": "Add comprehensive telemetry for token usage, generation duration, provider selection, and performance metrics",
            "dependencies": [
              4
            ],
            "details": "Implement telemetry tracking system that monitors token consumption, request duration, provider selection decisions, success/failure rates, and cost metrics. Create structured logging for debugging and analytics, integrate with existing monitoring infrastructure, and provide metrics for performance analysis and optimization.",
            "status": "pending",
            "testStrategy": "Token tracking accuracy validation, performance metrics collection testing, and monitoring system integration"
          },
          {
            "id": 6,
            "title": "Replace existing service calls and validate migration",
            "description": "Replace the existing custom fetch-based question generation with the new AI SDK service and validate complete migration",
            "dependencies": [
              5
            ],
            "details": "Update all existing service calls to use the new questionGenerationService, remove custom fetch() implementations and retry logic, and validate that the new implementation maintains feature parity. Ensure proper error handling, response formatting, and integration with the existing UI components and data flow.",
            "status": "pending",
            "testStrategy": "End-to-end integration testing, performance benchmarking against current implementation, and feature parity validation"
          }
        ]
      },
      {
        "id": 4,
        "title": "Build Blueprint Generation Service with Streaming",
        "description": "Rewrite blueprint generation service using AI SDK streamObject() with Perplexity primary and Ollama fallback",
        "details": "Create `frontend/lib/ai/services/blueprintGenerationService.ts` implementing: 1) Use AI SDK's streamObject() with Perplexity Sonar Deep Research as primary provider via custom OpenAI-compatible adapter. 2) Support streaming and non-streaming modes with proper context extraction from static/dynamic answers. 3) Fallback to existing Ollama wrapper on Perplexity failures. 4) Integrate with blueprint schema validation and structured outputs. 5) Migrate prompt building from existing blueprintGenerationService.ts and buildBlueprintPrompt(). 6) Handle V2.0 and legacy static answer formats for backward compatibility. 7) Implement comprehensive logging, telemetry, and error recovery. Replace triple-fallback logic (Sonnet→Opus→Ollama) with simplified Perplexity→Ollama approach.",
        "testStrategy": "Integration tests covering both modes, fallback testing, schema validation, context extraction from different static answer formats, token usage tracking, and performance comparison with existing service.",
        "priority": "high",
        "dependencies": [
          2
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Create Perplexity OpenAI-compatible adapter with Sonar Deep Research",
            "description": "Build custom Perplexity provider using OpenAI adapter to integrate Sonar Deep Research model",
            "dependencies": [],
            "details": "Create `frontend/lib/ai/providers/perplexityAdapter.ts` that wraps Perplexity API with OpenAI-compatible interface. Configure for Sonar Deep Research model, handle authentication, request/response mapping, and error handling. Ensure compatibility with AI SDK's streamObject() expectations.",
            "status": "pending",
            "testStrategy": "Unit tests for adapter initialization, API call mapping, error handling, and integration tests with actual Perplexity API calls"
          },
          {
            "id": 2,
            "title": "Implement context extraction from static and dynamic answers",
            "description": "Build context extraction system supporting both V2.0 and legacy static answer formats",
            "dependencies": [
              1
            ],
            "details": "Create context extraction utilities in `frontend/lib/ai/utils/contextExtractor.ts` that parse static_answers and dynamic_answers JSONB fields, handle V2.0 structured format and legacy flat format, and build comprehensive context for blueprint generation prompts.",
            "status": "pending",
            "testStrategy": "Unit tests with sample data from both formats, edge case handling, and validation of extracted context completeness"
          },
          {
            "id": 3,
            "title": "Build streaming and non-streaming blueprint generation modes",
            "description": "Implement dual-mode blueprint generation service with AI SDK streamObject integration",
            "dependencies": [
              2
            ],
            "details": "Create core service in `frontend/lib/ai/services/blueprintGenerationService.ts` with streamObject() for real-time streaming and generateObject() for batch processing. Include mode selection, progress tracking, and consistent output formatting for both approaches.",
            "status": "pending",
            "testStrategy": "Integration tests for both modes, output consistency validation, performance comparison, and streaming interrupt handling"
          },
          {
            "id": 4,
            "title": "Implement Perplexity to Ollama fallback mechanism",
            "description": "Build robust fallback system from Perplexity primary to Ollama secondary provider",
            "dependencies": [
              3
            ],
            "details": "Implement fallback logic in blueprintGenerationService.ts that detects Perplexity failures (API errors, rate limits, timeouts), gracefully switches to existing Ollama wrapper, maintains consistent schema validation, and logs fallback events for monitoring.",
            "status": "pending",
            "testStrategy": "Failure scenario testing with mocked Perplexity errors, fallback timing validation, output consistency between providers, and logging verification"
          },
          {
            "id": 5,
            "title": "Migrate prompt building and schema validation",
            "description": "Extract and integrate existing prompt building logic with new blueprint schema validation",
            "dependencies": [
              4
            ],
            "details": "Migrate buildBlueprintPrompt() from existing blueprintGenerationService.ts, integrate with AI SDK schema validation using experimental_output, ensure backward compatibility with existing blueprint structure, and maintain all 11 blueprint sections.",
            "status": "pending",
            "testStrategy": "Schema validation testing with existing blueprint data, prompt quality comparison, section completeness verification, and migration accuracy tests"
          },
          {
            "id": 6,
            "title": "Add comprehensive logging, telemetry, and error recovery",
            "description": "Implement structured logging, performance metrics, and error recovery for the blueprint generation service",
            "dependencies": [
              5
            ],
            "details": "Add structured logging throughout the service using existing patterns, implement telemetry for token usage, generation times, and success rates, build error recovery mechanisms for partial failures, and create health monitoring for both Perplexity and Ollama providers.",
            "status": "pending",
            "testStrategy": "Logging output validation, telemetry accuracy testing, error recovery scenario testing, and health check reliability verification"
          }
        ]
      },
      {
        "id": 5,
        "title": "Update API Routes for Streaming Support",
        "description": "Migrate API routes to use new AI services with Server-Sent Events streaming capability",
        "details": "Update two API routes: 1) `frontend/app/api/generate-dynamic-questions/route.ts` - Replace existing service calls with new questionGenerationService, add streaming parameter support, implement SSE response headers for streaming mode, maintain existing validation and error handling patterns. 2) `frontend/app/api/blueprints/generate/route.ts` - Replace existing service with new blueprintGenerationService, add streaming support, preserve usage limit checks and blueprint saving logic, maintain V2.0 static answer compatibility. Both routes must support feature flags, maintain exact same API contracts for backward compatibility, and include proper Zod request validation. Preserve existing maxDuration settings and authentication patterns.",
        "testStrategy": "API integration tests for both streaming and non-streaming endpoints, backward compatibility testing, error handling validation, feature flag testing, and authentication/authorization verification.",
        "priority": "medium",
        "dependencies": [
          3,
          4
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Update generate-dynamic-questions API route for streaming support",
            "description": "Migrate the dynamic questions generation API route to use the new questionGenerationService with Server-Sent Events streaming capability while maintaining backward compatibility",
            "dependencies": [],
            "details": "Replace existing service calls in `frontend/app/api/generate-dynamic-questions/route.ts` with new questionGenerationService. Add streaming parameter support to request validation schema. Implement SSE response headers for streaming mode using proper Next.js streaming response patterns. Maintain existing Zod validation, error handling patterns, authentication checks, and maxDuration settings. Ensure exact API contract compatibility for non-streaming mode.",
            "status": "pending",
            "testStrategy": "API integration tests for both streaming and non-streaming modes, request validation testing, SSE header verification, backward compatibility testing with existing clients, and error handling validation"
          },
          {
            "id": 2,
            "title": "Update blueprints generate API route for streaming support",
            "description": "Migrate the blueprint generation API route to use the new blueprintGenerationService with streaming capability while preserving all existing functionality",
            "dependencies": [
              1
            ],
            "details": "Replace existing service calls in `frontend/app/api/blueprints/generate/route.ts` with new blueprintGenerationService. Add streaming support with proper SSE implementation. Preserve all usage limit checks, blueprint saving logic, and V2.0 static answer compatibility. Maintain authentication patterns, maxDuration settings, and existing error handling. Implement feature flag support for gradual rollout while ensuring exact API contract preservation.",
            "status": "pending",
            "testStrategy": "Integration tests for streaming and non-streaming endpoints, usage limit validation, blueprint saving verification, V2.0 compatibility testing, and authentication/authorization checks"
          },
          {
            "id": 3,
            "title": "Implement Server-Sent Events response handling",
            "description": "Add proper Server-Sent Events response formatting and streaming infrastructure to both API routes for real-time progress updates",
            "dependencies": [
              1,
              2
            ],
            "details": "Implement SSE response formatting in both API routes using Next.js streaming capabilities. Add proper event data formatting with progress indicators, section updates, and completion events. Implement connection management, proper cleanup on client disconnect, and error event propagation. Ensure CORS headers are properly set for SSE connections and implement heartbeat mechanisms to maintain connection stability.",
            "status": "pending",
            "testStrategy": "SSE connection testing, event data format validation, connection stability testing, client disconnect handling, and cross-origin request verification"
          },
          {
            "id": 4,
            "title": "Add feature flag integration and backward compatibility validation",
            "description": "Integrate feature flags for streaming functionality and validate complete backward compatibility with existing API consumers",
            "dependencies": [
              1,
              2,
              3
            ],
            "details": "Add feature flag integration to both API routes to control streaming functionality rollout. Implement runtime switching between old and new service implementations based on feature flags. Create comprehensive backward compatibility validation ensuring existing API consumers continue to work without changes. Add monitoring and logging for feature flag usage and performance metrics comparison between old and new implementations.",
            "status": "pending",
            "testStrategy": "Feature flag toggle testing, backward compatibility validation with existing clients, performance comparison testing, monitoring data collection verification, and gradual rollout simulation"
          }
        ]
      },
      {
        "id": 6,
        "title": "Create Streaming UI Components",
        "description": "Build React components for real-time generation progress feedback using Server-Sent Events",
        "details": "Create `frontend/components/blueprint/GenerationProgress.tsx` implementing: 1) EventSource consumption from API streaming endpoints with proper connection management. 2) Real-time progress tracking showing current section being generated. 3) Visual progress bar with smooth animations using Framer Motion. 4) Status indicators for streaming, complete, and error states with appropriate icons. 5) Event logging for development debugging with collapsible details. 6) Proper cleanup and error handling for SSE connections. 7) Type-safe event handling for different stream event types (section, question, complete, error). Component should be reusable for both question and blueprint generation with configurable callbacks.",
        "testStrategy": "Component testing with React Testing Library, SSE connection mocking, progress state transitions, error boundary testing, and manual testing with real streaming endpoints.",
        "priority": "medium",
        "dependencies": [
          5
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Create EventSource Hook with Connection Management",
            "description": "Build a custom React hook to manage EventSource connections with proper lifecycle management, reconnection logic, and cleanup",
            "dependencies": [],
            "details": "Create `frontend/lib/hooks/useEventSource.ts` implementing: 1) Custom hook accepting URL and options for EventSource creation. 2) Connection state management (connecting, connected, disconnected, error). 3) Automatic reconnection with exponential backoff strategy. 4) Proper cleanup on unmount and dependency changes. 5) TypeScript interfaces for event data and connection states. 6) Error boundary integration for connection failures.",
            "status": "pending",
            "testStrategy": "Unit tests for hook lifecycle, connection state transitions, cleanup behavior, and reconnection logic with mocked EventSource"
          },
          {
            "id": 2,
            "title": "Implement Progress Tracking State Management",
            "description": "Create state management system for tracking generation progress with section-level granularity and status updates",
            "dependencies": [
              1
            ],
            "details": "Create progress state management in `frontend/components/blueprint/GenerationProgress.tsx`: 1) State interface for current section, completed sections, total progress percentage. 2) Event handlers for different SSE event types (section, question, complete, error). 3) Progress calculation logic based on section completion. 4) Status tracking for streaming, complete, and error states. 5) TypeScript types for all event payloads and progress states.",
            "status": "pending",
            "testStrategy": "Unit tests for progress calculations, state transitions, and event handling with various SSE event scenarios"
          },
          {
            "id": 3,
            "title": "Build Animated Progress Bar Component",
            "description": "Create visual progress bar with smooth animations using Framer Motion for real-time progress feedback",
            "dependencies": [
              2
            ],
            "details": "Implement animated progress bar: 1) Framer Motion progress bar with smooth transitions between progress states. 2) Section-based progress indicators with visual milestones. 3) Color coding for different states (streaming=blue, complete=green, error=red). 4) Responsive design working on mobile and desktop. 5) Accessibility attributes for screen readers. 6) Performance optimization for smooth 60fps animations.",
            "status": "pending",
            "testStrategy": "Component tests for animation behavior, accessibility compliance, responsive design, and performance under rapid state changes"
          },
          {
            "id": 4,
            "title": "Create Status Indicators and Icon System",
            "description": "Build status indicator components with appropriate icons for streaming, complete, and error states",
            "dependencies": [
              2
            ],
            "details": "Create status indicator system: 1) Icon components for each state using Lucide React icons (loader for streaming, check for complete, alert for error). 2) Status text with clear messaging for each state. 3) Loading spinner animation for streaming state. 4) Error message display with retry functionality. 5) Success confirmation with completion timestamp. 6) Consistent styling with existing design system.",
            "status": "pending",
            "testStrategy": "Component tests for icon rendering, state transitions, error message display, and user interaction handling"
          },
          {
            "id": 5,
            "title": "Add Development Logging and Error Handling",
            "description": "Implement comprehensive logging system with collapsible debug details and robust error handling for SSE connections",
            "dependencies": [
              1
            ],
            "details": "Create logging and error handling: 1) Event logging system capturing all SSE events with timestamps. 2) Collapsible debug panel showing connection status, events, and errors (development only). 3) Error boundary for SSE connection failures with user-friendly messages. 4) Connection retry logic with user feedback. 5) Cleanup handling for component unmount and route changes. 6) Performance monitoring for EventSource memory usage.",
            "status": "pending",
            "testStrategy": "Error boundary testing, memory leak detection, connection failure scenarios, and debug panel functionality in development mode"
          }
        ]
      },
      {
        "id": 7,
        "title": "Implement Feature Flags and Configuration",
        "description": "Add feature flags for gradual rollout and A/B testing of new AI SDK implementation",
        "details": "Extend `frontend/lib/ai/config.ts` with feature flag support: 1) Add environment variable controls for NEXT_PUBLIC_USE_VERCEL_AI_SDK, NEXT_PUBLIC_ENABLE_STREAMING, and NEXT_PUBLIC_ENABLE_OLLAMA_FALLBACK. 2) Update API routes to respect feature flags and route to old or new services accordingly. 3) Add runtime configuration switching without code changes. 4) Implement gradual rollout logic (percentage-based user cohorts). 5) Add monitoring hooks for feature flag usage analytics. 6) Create admin interface for real-time flag toggling. 7) Document flag usage and rollback procedures. Ensure seamless switching between old and new implementations without user disruption.",
        "testStrategy": "Feature flag toggle testing, percentage rollout validation, admin interface functionality, rollback procedure testing, and monitoring data accuracy verification.",
        "priority": "medium",
        "dependencies": [
          5
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Create feature flag configuration system with environment controls",
            "description": "Implement the core feature flag system with environment variable controls for AI SDK migration flags",
            "dependencies": [],
            "details": "Extend `frontend/lib/ai/config.ts` to add environment variable controls for NEXT_PUBLIC_USE_VERCEL_AI_SDK, NEXT_PUBLIC_ENABLE_STREAMING, and NEXT_PUBLIC_ENABLE_OLLAMA_FALLBACK. Create feature flag evaluation logic with runtime configuration switching capabilities. Implement configuration loader that reads environment variables and provides typed feature flag interface. Add validation for flag combinations and default fallback values.",
            "status": "pending",
            "testStrategy": "Unit tests for configuration loading, environment variable parsing, flag validation, and default value handling. Integration tests for runtime configuration switching."
          },
          {
            "id": 2,
            "title": "Implement percentage-based user cohort rollout system",
            "description": "Build gradual rollout logic with percentage-based user assignment and cohort management",
            "dependencies": [
              1
            ],
            "details": "Create user cohort assignment system using deterministic hashing based on user ID to ensure consistent feature flag experience. Implement percentage-based rollout logic that can gradually increase feature adoption from 0% to 100%. Add cohort tracking and analytics hooks for monitoring feature flag usage. Include rollback capabilities and emergency override switches. Ensure thread-safe operations and consistent user experience across sessions.",
            "status": "pending",
            "testStrategy": "Unit tests for cohort assignment consistency, percentage distribution accuracy, and rollback functionality. Load testing for concurrent user assignment and feature flag evaluation performance."
          },
          {
            "id": 3,
            "title": "Build admin interface for real-time feature flag management",
            "description": "Create admin dashboard for monitoring and controlling feature flags with real-time updates",
            "dependencies": [
              1,
              2
            ],
            "details": "Create admin interface at `frontend/app/(auth)/admin/feature-flags/` with real-time flag toggling capabilities. Implement dashboard showing current flag states, user cohort distributions, usage analytics, and rollout progress. Add role-based access control ensuring only developer tier users can access. Include audit logging for all flag changes, emergency rollback buttons, and monitoring dashboards. Update API routes to respect feature flags and route between old/new implementations seamlessly.",
            "status": "pending",
            "testStrategy": "UI testing for admin interface functionality, real-time update validation, role-based access verification, audit log accuracy, and emergency rollback procedure testing. End-to-end testing for seamless routing between implementations."
          }
        ]
      },
      {
        "id": 8,
        "title": "Write Comprehensive Test Suite",
        "description": "Create extensive test coverage for new AI integration including unit, integration, and performance tests",
        "details": "Implement comprehensive testing: 1) Unit tests for all providers, services, and utilities with 85%+ coverage. 2) Integration tests for end-to-end question and blueprint generation flows. 3) Performance benchmarks comparing old vs new implementation for speed and token usage. 4) Fallback scenario testing with mocked provider failures. 5) Schema validation testing with edge cases and malformed data. 6) Streaming functionality tests with SSE mocking. 7) Load testing for concurrent generation requests. 8) Create test fixtures with real production data samples. Use Vitest for all testing with proper mocking of external API calls. Include performance regression detection and automated benchmark reporting.",
        "testStrategy": "Self-testing: Automated test suite execution, coverage reporting, performance baseline establishment, CI/CD integration, and test data management.",
        "priority": "medium",
        "dependencies": [
          6
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Set up testing infrastructure and configuration",
            "description": "Configure Vitest testing framework with proper setup for AI SDK testing, mocking configuration, and coverage reporting",
            "dependencies": [],
            "details": "Create comprehensive test setup: 1) Configure Vitest with TypeScript and Next.js integration in frontend/vitest.config.ts. 2) Set up test environment with proper mocking for external APIs (Anthropic, Perplexity, Ollama). 3) Configure coverage reporting with 85%+ target thresholds. 4) Create shared test utilities and helpers in __tests__/utils/. 5) Set up test database configuration for integration tests. 6) Configure CI/CD integration for automated test execution.",
            "status": "pending",
            "testStrategy": "Self-validating through successful test suite execution and coverage report generation"
          },
          {
            "id": 2,
            "title": "Write unit tests for AI provider configurations",
            "description": "Create comprehensive unit tests for all three AI providers (Anthropic, Perplexity, Ollama) with proper mocking",
            "dependencies": [
              1
            ],
            "details": "Implement unit tests for: 1) Test anthropicProvider.ts initialization, model selection, and error handling. 2) Test perplexityProvider.ts custom OpenAI adapter configuration and API calls. 3) Test ollamaProvider.ts wrapper functionality and existing client integration. 4) Mock external API calls using Vitest mocks. 5) Test environment variable validation and error scenarios. 6) Achieve 90%+ coverage for provider code.",
            "status": "pending",
            "testStrategy": "Unit test execution with coverage reporting, mock verification, and error scenario validation"
          },
          {
            "id": 3,
            "title": "Create integration tests for generation services",
            "description": "Build end-to-end integration tests for both question generation and blueprint generation services with real AI SDK flow",
            "dependencies": [
              1,
              2
            ],
            "details": "Develop integration tests: 1) Test complete question generation flow from static answers to dynamic questions using AI SDK. 2) Test blueprint generation service with streaming and non-streaming modes. 3) Test fallback scenarios with mocked provider failures. 4) Validate schema compliance for all generated outputs. 5) Test context extraction from different static answer formats. 6) Include real production data samples as test fixtures.",
            "status": "pending",
            "testStrategy": "Integration test suite with real API mocking, schema validation, and end-to-end flow verification"
          },
          {
            "id": 4,
            "title": "Implement streaming functionality tests",
            "description": "Create specialized tests for Server-Sent Events streaming with proper SSE mocking and real-time progress validation",
            "dependencies": [
              1,
              3
            ],
            "details": "Build streaming tests: 1) Mock SSE connections using custom EventSource mocks. 2) Test streaming progress updates and real-time UI feedback. 3) Test connection error handling and reconnection logic. 4) Validate streaming data format and progressive content delivery. 5) Test streaming cancellation and cleanup. 6) Create React Testing Library tests for streaming UI components.",
            "status": "pending",
            "testStrategy": "SSE mock validation, streaming component testing, and real-time progress verification"
          },
          {
            "id": 5,
            "title": "Build performance benchmark suite",
            "description": "Create comprehensive performance tests comparing old vs new implementation for speed, token usage, and resource consumption",
            "dependencies": [
              1,
              3
            ],
            "details": "Implement performance benchmarks: 1) Create baseline measurements for existing Claude/Perplexity implementation. 2) Build comparative benchmarks for new AI SDK implementation measuring generation time, token usage, and memory consumption. 3) Implement load testing for concurrent generation requests. 4) Create automated performance regression detection with configurable thresholds. 5) Generate detailed performance reports with visualizations.",
            "status": "pending",
            "testStrategy": "Performance baseline establishment, comparative benchmarking, and automated regression detection"
          },
          {
            "id": 6,
            "title": "Create schema validation and edge case tests",
            "description": "Build comprehensive tests for schema validation, malformed data handling, and edge case scenarios across all AI responses",
            "dependencies": [
              1,
              2
            ],
            "details": "Develop validation tests: 1) Test Zod schema validation for all AI response formats. 2) Create edge case scenarios with malformed, incomplete, or invalid AI responses. 3) Test error handling for schema validation failures. 4) Validate type safety and runtime validation alignment. 5) Test boundary conditions and extreme input scenarios. 6) Create comprehensive test fixtures covering all schema edge cases.",
            "status": "pending",
            "testStrategy": "Schema validation testing, edge case scenario validation, and error handling verification"
          },
          {
            "id": 7,
            "title": "Set up automated test reporting and CI integration",
            "description": "Configure automated test execution, coverage reporting, and CI/CD integration with performance regression detection",
            "dependencies": [
              1,
              2,
              3,
              4,
              5,
              6
            ],
            "details": "Implement test automation: 1) Configure GitHub Actions workflow for automated test execution on PRs and main branch. 2) Set up coverage reporting with automatic PR comments and status checks. 3) Integrate performance benchmarks into CI pipeline with regression detection. 4) Create automated test result reporting with detailed failure analysis. 5) Configure test data management and fixture updates. 6) Set up monitoring for test suite performance and reliability.",
            "status": "pending",
            "testStrategy": "CI/CD pipeline validation, automated reporting verification, and test suite reliability monitoring"
          }
        ]
      },
      {
        "id": 9,
        "title": "Create Migration Utilities and Monitoring",
        "description": "Build utilities for smooth migration including data validation, performance monitoring, and error tracking",
        "details": "Create migration and monitoring infrastructure: 1) `frontend/lib/ai/utils/migrationValidator.ts` - Validate output consistency between old and new implementations. 2) `frontend/lib/ai/utils/performanceMonitor.ts` - Track generation times, token usage, success rates, and cost metrics. 3) Enhanced logging with structured telemetry for debugging and analytics. 4) Data migration scripts for any schema changes or format updates. 5) Health check endpoints for provider availability monitoring. 6) Alert system for failure rate increases or performance degradation. 7) Usage analytics dashboard showing migration progress and system health. Include rollback triggers and automated fallback to old system on critical failures.",
        "testStrategy": "Migration validation accuracy, monitoring data collection, alert system functionality, health check reliability, and rollback procedure automation testing.",
        "priority": "medium",
        "dependencies": [
          8
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Create Migration Validation Utilities",
            "description": "Build validation tools to ensure output consistency between old and new AI implementations during migration",
            "dependencies": [],
            "details": "Create `frontend/lib/ai/utils/migrationValidator.ts` with functions to compare outputs from legacy Claude/Ollama services against new AI SDK implementations. Include schema validation, content similarity scoring, performance delta tracking, and automated test case generation. Implement side-by-side comparison tools for manual validation and automated regression detection.",
            "status": "pending",
            "testStrategy": "Unit tests for validation logic, integration tests with real service outputs, comparison accuracy testing, and automated regression detection validation"
          },
          {
            "id": 2,
            "title": "Build Performance Monitoring Infrastructure",
            "description": "Implement comprehensive performance tracking for AI generation times, token usage, success rates, and cost metrics",
            "dependencies": [
              1
            ],
            "details": "Create `frontend/lib/ai/utils/performanceMonitor.ts` with telemetry collection for generation times, token consumption, API response times, success/failure rates, and cost tracking across all providers. Include real-time metrics aggregation, historical data storage, and performance trend analysis. Integrate with existing logging infrastructure and provide dashboard-ready data exports.",
            "status": "pending",
            "testStrategy": "Metrics collection accuracy testing, performance overhead measurement, data aggregation validation, and integration testing with monitoring dashboards"
          },
          {
            "id": 3,
            "title": "Implement Health Check and Alert Systems",
            "description": "Create health monitoring endpoints and alert systems for provider availability and performance degradation detection",
            "dependencies": [
              2
            ],
            "details": "Build health check endpoints in `frontend/app/api/health/` for monitoring all AI providers, database connections, and critical services. Implement alert system for failure rate increases, performance degradation, and provider unavailability. Include automated notifications, escalation procedures, and integration with existing monitoring infrastructure. Add circuit breaker patterns for graceful degradation.",
            "status": "pending",
            "testStrategy": "Health check endpoint testing, alert system functionality validation, circuit breaker testing, and failure simulation scenarios"
          },
          {
            "id": 4,
            "title": "Create Data Migration Scripts and Rollback Systems",
            "description": "Build migration scripts for schema changes and implement automated rollback triggers for critical failures",
            "dependencies": [
              1,
              2,
              3
            ],
            "details": "Create data migration scripts in `scripts/migration/` for any schema changes or format updates required by AI SDK integration. Implement rollback triggers that automatically fall back to the old system on critical failures. Include database migration utilities, configuration rollback procedures, and automated testing of rollback scenarios. Add migration progress tracking and safety checks.",
            "status": "pending",
            "testStrategy": "Migration script testing with production data samples, rollback procedure validation, automated failure detection testing, and end-to-end migration scenario testing"
          }
        ]
      },
      {
        "id": 10,
        "title": "Deploy and Clean Up Legacy Code",
        "description": "Execute production deployment with gradual rollout and remove deprecated code after validation",
        "details": "Complete migration deployment: 1) Deploy with feature flags disabled initially for staging validation. 2) Execute gradual production rollout (10% → 50% → 100% users) with monitoring. 3) Validate production metrics match or exceed baseline performance. 4) Mark legacy services as deprecated with proper TypeScript annotations and warnings. 5) After 2-week grace period, remove old code: claude/ directory, dynamicQuestionGenerationV2.ts, perplexityQuestionService.ts, old blueprintGenerationService.ts. 6) Update documentation, CLAUDE.md, and API docs. 7) Clean up unused dependencies and imports. 8) Verify CI/CD pipeline passes without old code references. Ensure complete migration with zero production issues and successful legacy code removal.",
        "testStrategy": "Production deployment verification, rollout metrics validation, deprecated code warning functionality, complete removal testing, documentation accuracy, and post-cleanup system stability confirmation.",
        "priority": "low",
        "dependencies": [
          9
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Deploy with Feature Flags and Execute Gradual Rollout",
            "description": "Deploy the migrated system to production with feature flags disabled initially, then execute gradual rollout (10% → 50% → 100% users) with comprehensive monitoring",
            "dependencies": [],
            "details": "1) Deploy to staging with feature flags disabled for initial validation. 2) Configure monitoring dashboards for performance metrics, error rates, and user experience indicators. 3) Execute gradual production rollout starting with 10% of users, monitoring key metrics at each stage. 4) Validate that production metrics match or exceed baseline performance before proceeding to next rollout percentage. 5) Complete rollout to 100% of users once all validation checkpoints pass successfully.",
            "status": "pending",
            "testStrategy": "Production deployment verification with staging validation, rollout metrics monitoring at each percentage threshold, performance baseline comparison testing, and user experience impact assessment."
          },
          {
            "id": 2,
            "title": "Mark Legacy Services as Deprecated with Proper Annotations",
            "description": "Add deprecation warnings and TypeScript annotations to legacy code components before removal to ensure proper transition period",
            "dependencies": [
              1
            ],
            "details": "1) Add @deprecated JSDoc annotations to legacy services in claude/ directory, dynamicQuestionGenerationV2.ts, perplexityQuestionService.ts, and old blueprintGenerationService.ts. 2) Implement runtime deprecation warnings for legacy service usage. 3) Update TypeScript interfaces to mark deprecated methods and properties. 4) Add console warnings when legacy code paths are executed. 5) Document migration paths and alternatives for each deprecated component. 6) Set up 2-week grace period monitoring to track deprecated code usage.",
            "status": "pending",
            "testStrategy": "Deprecated code warning functionality testing, TypeScript compilation validation with deprecation annotations, runtime warning verification, and usage tracking confirmation."
          },
          {
            "id": 3,
            "title": "Remove Legacy Code and Update Documentation",
            "description": "After validation period, completely remove deprecated legacy code, clean up dependencies, and update all documentation and configuration files",
            "dependencies": [
              2
            ],
            "details": "1) Remove legacy code directories and files: claude/ directory, dynamicQuestionGenerationV2.ts, perplexityQuestionService.ts, old blueprintGenerationService.ts. 2) Clean up unused dependencies from package.json and package-lock.json. 3) Remove unused imports across the codebase. 4) Update CLAUDE.md to reflect new architecture and remove legacy references. 5) Update API documentation to remove deprecated endpoints. 6) Verify CI/CD pipeline passes without old code references. 7) Run comprehensive test suite to ensure no broken imports or references.",
            "status": "pending",
            "testStrategy": "Complete removal testing with CI/CD pipeline validation, documentation accuracy verification, unused dependency cleanup confirmation, and post-cleanup system stability testing with full test suite execution."
          }
        ]
      }
    ],
    "metadata": {
      "created": "2025-10-25T08:46:39.782Z",
      "updated": "2025-10-25T08:46:39.783Z",
      "description": "Tasks for master context"
    }
  }
}